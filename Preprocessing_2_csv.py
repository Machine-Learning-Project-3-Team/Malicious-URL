import warnings
warnings.filterwarnings('ignore')
warnings.filterwarnings("ignore", category=UserWarning)
import os, re, zipfile, glob, time, gc, math, subprocess, random
import pandas as pd
import numpy as np
import requests
import tldextract

from io import BytesIO, StringIO
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample
from collections import Counter
from scipy.stats import zscore
from urllib.parse import urlparse, urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter

os.environ['KAGGLE_CONFIG_DIR'] = "/home/ubuntu"
download_dir = "/home/ubuntu"

# ë‹¤ìš´ íŒŒíŠ¸
kaggle_files = [
    ("sid321axn/malicious-urls-dataset", "malicious-urls-dataset.zip"),
    ("taruntiwarihp/phishing-site-urls", "phishing-site-urls.zip"),
    ("antonyj453/urldataset", "urldataset.zip"),
    ("pilarpieiro/tabular-dataset-ready-for-malicious-url-detection", "tabular-dataset-ready-for-malicious-url-detection.zip")
]

for dataset, zipname in kaggle_files:
    os.system(f"kaggle datasets download -d {dataset} -p {download_dir}")
    zip_path = os.path.join(download_dir, zipname)
    if os.path.exists(zip_path):
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(download_dir)
        os.remove(zip_path)


# ì•…ì„± url ë°ì´í„° ìˆ˜ì§‘
#malicious_url = "https://urlhaus.abuse.ch/downloads/text/"
#malicious_df = pd.read_csv(malicious_url, comment='#', header=None, names=['url'])
urlhaus_url = "https://urlhaus.abuse.ch/downloads/csv/"
response = requests.get(urlhaus_url)
response.raise_for_status()

# ì••ì¶• í•´ì œ + CSV ì½ê¸°
with zipfile.ZipFile(BytesIO(response.content)) as z:
    name = z.namelist()[0]
    with z.open(name) as f:
        df = pd.read_csv(f, encoding='latin1', header=None, comment='#', on_bad_lines='skip')

# ì—´ ì´ë¦„ ìˆ˜ë™ ì§€ì •
df.columns = ['id', 'dateadded', 'url', 'url_status', 'unused1', 'threat', 'unused2', 'urlhaus_link', 'signature']

# URLë§Œ ì¶”ì¶œ
malicious_df = df[['url']].dropna()


openphish_url = "https://openphish.com/feed.txt"
openphish_df = pd.read_csv(openphish_url, header=None, names=['url'])

# ìºê¸€ ë°ì´í„° ì½ê¸°
kaggle_1_df = pd.read_csv(f"{download_dir}/malicious_phish.csv")

kaggle_1_df.rename(columns={'type': 'label'}, inplace=True)

kaggle_1_df['label'] = kaggle_1_df['label'].map({'benign': 0, 'malware': 1, 'phishing': 2})


# kaggle_1_df['label'].value_counts()
# kaggle_3_df.columns

kaggle_2_df = pd.read_csv(f"{download_dir}/phishing_site_urls.csv")
kaggle_2_df.rename(columns={'URL': 'url', 'Label': 'label'}, inplace=True)
kaggle_2_df['label'] = kaggle_2_df['label'].map({'bad': 2, 'good': 0})

kaggle_3_df = pd.read_csv(f"{download_dir}/data.csv")
kaggle_3_df.rename(columns={'URL': 'url', 'Label': 'label'}, inplace=True)
kaggle_3_df['label'] = kaggle_3_df['label'].map({'bad': 2, 'good': 0})
kaggle_3_df = kaggle_3_df[kaggle_3_df['label'] != 2]

kaggle_4_df_1 = pd.read_csv(f"{download_dir}/train_dataset.csv", usecols=['url', 'label'])
kaggle_4_df_2 = pd.read_csv(f"{download_dir}/test_dataset.csv", usecols=['url', 'label'])

kaggle_4_df_1.rename(columns={'URL': 'url', 'Label': 'label'}, inplace=True)
kaggle_4_df_1['label'] = kaggle_4_df_1['label'].map({1: 2, 0: 0})
kaggle_4_df_1 = kaggle_4_df_1[kaggle_4_df_1['label'] != 2]

kaggle_4_df_2.rename(columns={'URL': 'url', 'Label': 'label'}, inplace=True)
kaggle_4_df_2['label'] = kaggle_4_df_2['label'].map({1: 2, 0: 0})
kaggle_4_df_2 = kaggle_4_df_2[kaggle_4_df_2['label'] != 2]

# í†µí•©
kaggle_df = pd.concat([kaggle_1_df, kaggle_2_df, kaggle_3_df, kaggle_4_df_1, kaggle_4_df_2], ignore_index=True)
kaggle_df['label'].value_counts()

# ì•…ì„± url ë°ì´í„°
malicious_df['label'] = 1 # ë©€ì›¨ì–´
openphish_df['label'] = 2 # í”¼ì‹±

# ë°ì´í„° í•©ì¹˜ê¸° (ì •ìƒ, ì•…ì„±)
df = pd.concat([malicious_df, openphish_df, kaggle_df], ignore_index=True)

print(df['label'].value_counts())

# ì–¸ë”ìƒ˜í”Œë§ ì§„í–‰
# 2. í´ë˜ìŠ¤ë³„ ë°ì´í„° ë¶„ë¦¬
df_normal = df[df['label'] == 0]
df_malicious = df[df['label'].isin([1, 2])]

# 3. ì•…ì„± ë°ì´í„° ê°œìˆ˜ í™•ì¸
n_malicious = len(df_malicious)

# 4. ì •ìƒ ë°ì´í„°ë¥¼ ì•…ì„± ë°ì´í„° ìˆ˜ë§Œí¼ ì–¸ë”ìƒ˜í”Œë§
df_normal_undersampled = resample(df_normal,
                                  replace=False,       # ë³µì› ì—†ì´
                                  n_samples=n_malicious, # ì•…ì„± ìˆ˜ë§Œí¼ë§Œ ë½‘ê¸°
                                  random_state=42)     # ì¬í˜„ ê°€ëŠ¥ì„±

# 5. ì–¸ë”ìƒ˜í”Œëœ ì •ìƒ ë°ì´í„°ì™€ ì•…ì„± ë°ì´í„° í•©ì¹˜ê¸°
df = pd.concat([df_normal_undersampled, df_malicious])

# 6. ì…”í”Œ (í–‰ ìˆœì„œë¥¼ ì„ê¸° ìœ„í•´)
df = df.sample(frac=1, random_state=42).reset_index(drop=True)



# ì¤‘ë³µ URL ì œê±°
df = df.drop_duplicates(subset=['url'])
print(df['label'].value_counts())


# === ì‚¬ìš©í•  HTML íƒœê·¸ ëª©ë¡
tags = ['script', 'iframe', 'form', 'input', 'meta', 'link', 'object', 'embed']
event_handlers = ['onclick', 'onmouseover', 'onload', 'onerror', 'onfocus', 'onblur']

redirect_patterns = [re.compile(p, re.IGNORECASE) for p in [
    r'window\.location', 
    r'location\.href', 
    r'location\.replace', 
    r'document\.location'
]]

# === ìƒˆ í”¼ì²˜ ì¶”ê°€ (ê¸°ì¡´ ì»¬ëŸ¼ ìœ ì§€)
df['url_available'] = 0
for tag in tags:
    df[f'html_{tag}_num'] = 0
for handler in event_handlers:
    df[f'html_event_{handler}_num'] = 0
    df[f'html_event_{handler}_elem'] = 0 # ì´ë²¤íŠ¸ì— ì—°ê²°ëœ ìš”ì†Œ ê°œìˆ˜
df['html_external_resource_num'] = 0
df['html_same_domain_ratio'] = 0
df['html_js_redirect_count'] = 0
df['html_redirected'] = 0
df['html_redirected_domain_changed'] = 0
if 'html_processed' not in df.columns:
    df['html_processed'] = 0
if 'url_checked' not in df.columns:
    df['url_checked'] = 0

# í…ŒìŠ¤íŠ¸ìš© ë‚˜ì¤‘ì— ì‚­ì œ
total_urls = len(df)
# df = df.head(1000)
start = time.time()
batch_size = 1000

# ì¤‘ê°„ì €ì¥ ë‹¤ì‹œì‹œì‘ ë¶€ë¶„
files = glob.glob('intermediate_1_*.csv')
if files:
    latest = max(files, key=lambda x: int(x.split('_')[2].split('.')[0]))
    print(f'â–¶ ì¤‘ê°„ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘: {latest}')
    df = pd.read_csv(latest)
    start_i = int(latest.split('_')[2].split('.')[0]) + batch_size
else:
    start_i = 0



# ì´ë™ëœ url ìˆ˜ì •
headers = {
    'User-Agent': 'Mozilla/5.0 (compatible; URLChecker/1.0)'
}

session = requests.Session()
session.headers.update(headers)
adapter = HTTPAdapter(pool_connections=100, pool_maxsize=100)
session.mount('http://', adapter)
session.mount('https://', adapter)

existing = set(df['url'])
new_rows = []
redirect_sources = set()
REDIRECT_STATUS = {301, 302, 307, 308}

alive_idxs = []
#max_workers = min(32, os.cpu_count() * 2)
max_workers = min(64, os.cpu_count() * 8)

for i in range(start_i, len(df), batch_size):
    batch = df.iloc[i:i+batch_size]
    batch = batch[batch['url_checked'] == 0]
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {
            ex.submit(
                lambda url=(row['url'].strip() if row['url'].strip().startswith('http')
                             else 'http://' + row['url'].strip()):
                    session.head(url, timeout=0.8, allow_redirects=False)
            ): idx
            for idx, row in batch.iterrows()
        }
        for fut in as_completed(futures):
            idx = futures[fut]
            try:
                r = fut.result()
                code = r.status_code
                df.loc[idx, 'url_checked'] = 1

                if code == 405:
                    r = session.get(r.url, timeout=0.8, allow_redirects=False)
                    code = r.status_code
                if code < 400:
                    df.loc[idx, 'url_available'] = 1
                    alive_idxs.append(idx)
            except Exception as e:
                df.loc[idx, 'url_checked'] = 1
                print(f"[HEAD ERROR][{idx}] {e}", flush=True)
    offset = start_i + i
    if i % (batch_size * 5) == 0:
        df.drop_duplicates(subset='url', inplace=True)
        done = df['url_checked'].sum()
        df.to_csv(f'intermediate_1_{int(done)}.csv', index=False)

# html í”¼ì²˜ ì¶”ê°€

# ì¤‘ê°„ì €ì¥
files_html = glob.glob('intermediate_html_*.csv')
if files_html:
    latest_html  = max(files_html, key=lambda x: int(x.split('_')[2].split('.')[0]))
    print(f'â–¶ ì¤‘ê°„ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘: {latest_html }')
    df = pd.read_csv(latest_html )
else:
    df['html_processed'] = 0

alive_idxs = df.index[df['url_available'] == 1].tolist()

# === ë³‘ë ¬ ìš”ì²­ ë° íŒŒì‹±
for offset in range(0, len(alive_idxs), batch_size):
    idxs = alive_idxs[offset:offset+batch_size]
    to_proc = [idx for idx in idxs if df.at[idx, 'html_processed'] == 0]
    results = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            (idx, executor.submit(
                lambda u=(df.at[idx,'url'].strip()
                           if df.at[idx,'url'].strip().startswith('http')
                           else 'http://' + df.at[idx,'url'].strip()):
                    session.get(u, timeout=0.6, allow_redirects=True)
            ))
            for idx in to_proc
        ]

        for idx, fut in futures:
            try:
                r = fut.result()
            except Exception as e:
                print(f"[HTML ERROR][{idx}] {e}", flush=True)
                continue

            if 300 <= r.status_code < 400 and 'Location' in r.headers:
                new_url = urljoin(r.url, r.headers['Location'])
                r = session.get(new_url, timeout=0.6, allow_redirects=True)
            
            # ë¦¬ë‹¤ì´ë ‰íŠ¸ ì½”ë“œ ê°ì§€
            soup_tmp  = BeautifulSoup(r.text, 'lxml')
            meta = soup_tmp.find('meta', attrs={'http-equiv': lambda v: v and v.lower()=='refresh'})
            if meta and 'url=' in meta.get('content','').lower():
                part = meta['content'].split('url=')[-1].strip().strip("'\"")
                new_url = urljoin(r.url, part)
                try:
                    r = session.get(new_url, timeout=0.6, allow_redirects=True)
                except:
                    pass

            soup = BeautifulSoup(r.text, 'lxml')
            text = r.text.lower()
            if 'moved' in text:
                link = soup.find('a', href=True)   
                if link:
                    new_url = urljoin(r.url, link['href'])
                    try:
                        r = session.get(urljoin(r.url, link['href']), timeout=0.6, allow_redirects=True)
                        soup = BeautifulSoup(r.text, 'lxml')
                    except:
                        pass

            # ë¦¬ë‹¤ì´ë ‰íŠ¸ ì—¬ë¶€ ë° ë„ë©”ì¸ ë³€ê²½
            redirected   = int(bool(r.history))
            dom_changed  = int(
                urlparse(df.at[idx,'url']).netloc.replace('www.','')
                != urlparse(r.url).netloc.replace('www.','')
            )

            
            html = str(soup)
            tag_counts   = [html.count(f'<{t}') for t in tags]
            event_counts = [html.count(h+'=') for h in event_handlers]
            event_elems  = [len(soup.find_all(attrs={h:True})) for h in event_handlers]

            origin = urlparse(df.at[idx,'url']).netloc.replace('www.','')
            ext = same = tot = 0
            for tag, attr in [('a','href'),('script','src'),('img','src'),('link','href')]:
                for e in soup.find_all(tag):
                    u2 = e.get(attr)
                    if not u2: continue
                    p2 = urlparse(u2); tot += 1
                    if not p2.netloc or origin in p2.netloc:
                        same += 1
                    else:
                        ext += 1
            js_redirects = sum(len(p.findall(html)) for p in redirect_patterns)

            results.append((
                idx, redirected, dom_changed,
                *tag_counts, *event_counts, *event_elems,
                ext, (same/tot if tot else 0), js_redirects
            ))
            del r, soup, html, soup_tmp
    gc.collect()  
    cols = (
        [f'html_{t}_num' for t in tags] +
        [f'html_event_{h}_num'  for h in event_handlers] +
        [f'html_event_{h}_elem' for h in event_handlers] +
        ['html_external_resource_num','html_same_domain_ratio','html_js_redirect_count']
    )
    processed = []
    for res in results:
        idx = res[0]
        processed.append(idx)
        df.at[idx, 'html_redirected']                = res[1]
        df.at[idx, 'html_redirected_domain_changed'] = res[2]
        for col, val in zip(cols, res[3:3+len(cols)]):
            df.at[idx, col] = val
        df.at[idx, 'html_processed'] = 1

    if i % (batch_size * 5) == 0: # ìˆ˜ì •: ì²˜ë¦¬ëœ í–‰ ì œê±°ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        done = df['html_processed'].sum()
        df.to_csv(f'intermediate_html_{int(done)}.csv', index=False)
    

end = time.time()
elapsed = end - start
print(elapsed)
print(elapsed/1000 * total_urls / 3600)

# url ê¸¸ì´ ì»¬ëŸ¼ ì¶”ê°€
df['url_length'] = df['url'].apply(len)

# ì (.) ê°œìˆ˜ ì»¬ëŸ¼ ì¶”ê°€
df['count_dots'] = df['url'].apply(lambda x: x.count('.'))

# ìˆ«ì ê°œìˆ˜ ì»¬ëŸ¼ ì¶”ê°€
df['count_digits'] = df['url'].apply(lambda x: sum(c.isdigit() for c in x))

# íŠ¹ìˆ˜ë¬¸ì ê°œìˆ˜ ì»¬ëŸ¼ ì¶”ê°€
special_chars = "-_=+&%$#@!*"
df['count_special'] = df['url'].apply(lambda x: sum(c in special_chars for c in x))

# ìŠ¬ë˜ì‹œ(/) ê°œìˆ˜
df['count_slash'] = df['url'].apply(lambda x: x.count('/'))

# ë¬¼ìŒí‘œ(?) í¬í•¨ ì—¬ë¶€
df['count_question'] = df['url'].apply(lambda x: x.count('?'))

# ë“±í˜¸(=) ê°œìˆ˜
df['count_equal'] = df['url'].apply(lambda x: x.count('='))

# í¼ì„¼íŠ¸(%) ê°œìˆ˜
df['count_percent'] = df['url'].apply(lambda x: x.count('%'))

# ì¸ì½”ë”© ì—¬ë¶€ (%3f ê°™ì€ê±°)
df['is_encoded'] = df['url'].apply(lambda x: 1 if re.search(r'%[0-9a-fA-F]{2}', x) else 0)

# ì˜ì‹¬ TLD í¬í•¨ ì—¬ë¶€  TLD ex) .com
suspicious_tlds = ['zip', 'top', 'xyz', 'tk', 'ml']
df['suspicious_tld'] = df['url'].apply(lambda x: 1 if x.split('.')[-1].lower() in suspicious_tlds else 0)

# í™•ì¥ì ì—¬ë¶€ (exe php sh ë“±ë“±)

# IP ì£¼ì†Œ í¬í•¨ ì—¬ë¶€
df['has_ip'] = df['url'].apply(lambda x: 1 if re.search(r'://\d+\.\d+\.\d+\.\d+', x) else 0)

# tldextract ê¸°ë°˜ ë„ë©”ì¸ ì •ë³´ ê¸¸ì´ (ê° ë„ë©”ì¸ ì •ë³´)
ext = df['url'].apply(lambda x: tldextract.extract(x, include_psl_private_domains=True))

df['subdomain_len'] = ext.apply(lambda x: len(x.subdomain))
df['domain_len'] = ext.apply(lambda x: len(x.domain))
df['suffix_len'] = ext.apply(lambda x: len(x.suffix))

# ì„œë¸Œ ë„ë©”ì¸ êµ¬ì„± ê°œìˆ˜
df['count_subdomain_parts'] = ext.apply(lambda x: len(x.subdomain.split('.')) if x.subdomain else 0)

# https í¬í•¨ ì—¬ë¶€
df['has_https'] = df['url'].apply(lambda x: 1 if 'https://' in x else 0)

# í™•ì¥ì ì—¬ë¶€ (exe php sh ë“±ë“±)
df['file_ext'] = df['url'].apply(
    lambda x: re.findall(r'\.([a-zA-Z0-9]{1,6})(?:[/?#]|$)', x)
)
df['file_ext'] = df['file_ext'].apply(lambda ext: ext[-1].lower() if ext else 'none')

dangerous_exts = ['exe', 'php', 'asp', 'aspx', 'jsp', 'scr', 'bat', 'sh']
df['suspicious_ext'] = df['file_ext'].apply(lambda x: 1 if x in dangerous_exts else 0)
df.drop(columns='file_ext', inplace=True)
# ê²½ë¡œ ê¹Šì´(path depth)
df['path_depth'] = df['url'].apply(lambda x: len([p for p in urlparse(x).path.split('/') if p]))

# URL ë¬¸ì ì—”íŠ¸ë¡œí”¼ (ë³µì¡ë„) ë‚®ì„ìˆ˜ë¡ ë³µì¡í•œ urlì´ ì•„ë‹˜
df['url_entropy'] = df['url'].apply(
    lambda s: -sum(
        (s.count(c) / len(s)) * math.log2(s.count(c) / len(s))
        for c in dict.fromkeys(s)
    ) if len(s) > 0 else 0
)

# url ë‹¨ì¶• ì—¬ë¶€
shorteners = ['bit.ly', 't.co', 'tinyurl.com', 'goo.gl', 'ow.ly', 'buff.ly', 'rebrand.ly', 'is.gd', 'u.to', 'shorte.st']
df['html_is_shortened'] = df['url'].apply(lambda x: 1 if any(s in x for s in shorteners) else 0)

# ì—¬ê¸°ë¶€í„° ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ì½”ë“œ

# ë¼ë²¨ ë¶„ë¦¬ (label ë°ì´í„° ë³€í˜• ë°©ì§€)
url = df['url']
y = df['label']
X = df.drop(columns=['url', 'label'])

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
null_ratio = X.isnull().mean()

# 10% ~ 50% â†’ í‰ê·  ëŒ€ì¹˜
#fill_cols = null_ratio[(null_ratio >= 0.1) & (null_ratio < 0.5)].index.tolist()
#for col in fill_cols:
#    X[col].fillna(X[col].mean(), inplace=True)

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - 0ì¸ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ì–´ ì œê±°í•˜ì§€ ì•Šê³  -1 ë¡œ ëŒ€ì²´í•¨
# ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ë„ ì˜ë¯¸ìˆëŠ” feature ì¼ ìˆ˜ ìˆìŒ
X = X.fillna(-1)
url = url.loc[X.index]
y = y.loc[X.index]


# í‘œì¤€í™”(ì •ê·œí™”) : í•™ìŠµ ëª¨ë¸ì´ ì •ê·œí™”ëœ ê°’ì„ ë” ì˜ ì‚¬ìš©í•´ì„œ ì‚¬ìš©í•¨
#scaler = StandardScaler()
#X_scaled = scaler.fit_transform(X)

# ë³µì›
#df_cleaned = pd.DataFrame(X_scaled, columns=X.columns)
df_cleaned = X.copy()
df_cleaned['label'] = y.reset_index(drop=True)
df_cleaned['url'] = url.reset_index(drop=True)

# ìµœì¢… ë°ì´í„° ì €ì¥
# df_cleaned.to_csv('cleaned_url_dataset.csv', index=False)

# í™•ì¥ì í¬í•¨
# (ë…¼ë¬¸ì—ì„œ ì•…ì„± url featureê°€ ìˆì„ ìˆ˜ ìˆìŒ)
#
print(df_cleaned['label'].value_counts())
print(df.dtypes)


df = df_cleaned
#df = df_balanced


# 8. âœ… íŒŒì¼ë¡œ ì €ì¥
df.to_csv('balanced_data.csv', index=False)

# ì œëŒ€ë¡œ ë“¤ì–´ê°”ë‚˜ í™•ì¸
df = pd.read_csv('balanced_data.csv')  # ê²½ë¡œëŠ” ìƒí™©ì— ë§ê²Œ ìˆ˜ì •
print("ğŸ“Š balanced_data.csv ë‚´ í´ë˜ìŠ¤ë³„ ë¶„í¬:")
print(df['label'].value_counts())